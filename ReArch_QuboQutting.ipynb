{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f5db90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debd839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import required for random matrix generation\n",
    "import scipy.stats as stats\n",
    "import scipy.sparse as sparse\n",
    "from docplex.cp.utils_visu import display\n",
    "\n",
    "from qiskit import *\n",
    "from qiskit_optimization.applications import Maxcut, Tsp, GraphPartition\n",
    "\n",
    "# QP specific imports\n",
    "from qiskit_optimization import QuadraticProgram\n",
    "from qiskit_optimization.converters import QuadraticProgramToQubo, LinearInequalityToPenalty\n",
    "\n",
    "# QAOA and circuit cutting specific imports\n",
    "from qiskit.circuit.library import QAOAAnsatz\n",
    "#from circuit_knitting_toolbox.circuit_cutting.wire_cutting import cut_circuit_wires\n",
    "from circuit_knitting.cutting import cut_gates, cut_wires, cutqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e5aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, qpy\n",
    "from qiskit.quantum_info import PauliList\n",
    "from qiskit_aer.primitives import Estimator, Sampler\n",
    "from qiskit.circuit import Parameter\n",
    "\n",
    "from circuit_knitting.cutting import (\n",
    "    partition_problem,\n",
    "    generate_cutting_experiments,\n",
    "    reconstruct_expectation_values,\n",
    ")\n",
    "\n",
    "from circuit_knitting.cutting import cut_gates, cut_wires, cutqc, expand_observables\n",
    "from circuit_knitting.cutting.instructions import CutWire, Move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b0a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "from matplotlib import pylab as pl\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "import markov_clustering as mc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#community detection\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "import community as community_louvain\n",
    "from collections import defaultdict\n",
    "from string import ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dac948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful additional packages\n",
    "from qiskit_aer import Aer\n",
    "from qiskit.tools.visualization import plot_histogram\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "from qiskit_optimization.applications import Maxcut, Tsp\n",
    "from qiskit.algorithms.minimum_eigensolvers import SamplingVQE, NumPyMinimumEigensolver\n",
    "from qiskit.algorithms.optimizers import SPSA\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_optimization.algorithms import MinimumEigenOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "987b23e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĀāĂăĄąĆćĈĉĊċČčĎďĐđĒēĔĕĖėĘęĚěĜĝĞğĠġĢģĤĥĦħĨĩĪīĬĭĮįİıĲĳĴĵĶķĸĹĺĻļĽľĿŀŁłŃńŅņŇňŉŊŋŌōŎŏŐőŒœŔŕŖŗŘřŚśŜŝŞşŠšŢţŤťŦŧŨũŪūŬŭŮůŰűŲųŴŵŶŷŸŹźŻżŽžſƀƁƂƃƄƅƆƇƈƉƊƋƌƍƎƏƐƑƒƓƔƕƖƗƘƙƚƛƜƝƞƟƠơƢƣƤƥƦƧƨƩƪƫƬƭƮƯưƱƲƳƴƵƶƷƸƹƺƻƼƽƾƿǀǁǂǃǄǅǆǇǈǉǊǋǌǍǎǏǐǑǒǓǔǕǖǗǘǙǚǛǜǝǞǟǠǡǢǣǤǥǦǧǨǩǪǫǬǭǮǯǰǱǲǳǴǵǶǷǸǹǺǻǼǽǾǿȀȁȂȃȄȅȆȇȈȉȊȋȌȍȎȏȐȑȒȓȔȕȖȗȘșȚțȜȝȞȟȠȡȢȣȤȥȦȧȨȩȪȫȬȭȮȯȰȱȲȳȴȵȶȷȸȹȺȻȼȽȾȿɀɁɂɃɄɅɆɇɈɉɊɋɌɍɎɏɐɑɒɓɔɕɖɗɘəɚɛɜɝɞɟɠɡɢɣɤɥɦɧɨɩɪɫɬɭɮɯɰɱɲɳɴɵɶɷɸɹɺɻɼɽɾɿʀʁʂʃʄʅʆʇʈʉʊʋʌʍʎʏʐʑʒʓʔʕʖʗʘʙʚʛʜʝʞʟʠʡʢʣʤʥʦʧʨʩʪʫʬʭʮʯʰʱʲʳʴʵʶʷʸʹʺʻʼʽʾʿˀˁ˂˃˄˅ˆˇˈˉˊˋˌˍˎˏːˑ˒˓˔˕˖˗˘˙˚˛˜˝˞˟ˠˡˢˣˤ˥˦˧˨˩˪˫ˬ˭ˮ˯˰˱˲˳˴˵˶˷˸˹˺˻˼˽˾˿̴̵̶̷̸̡̢̧̨̛̖̗̘̙̜̝̞̟̠̣̤̥̦̩̪̫̬̭̮̯̰̱̲̳̹̺̻̼͇͈͉͍͎̀́̂̃̄̅̆̇̈̉̊̋̌̍̎̏̐̑̒̓̔̽̾̿̀́͂̓̈́͆͊͋͌̕̚ͅ͏͓͔͕͖͙͚͐͑͒͗͛ͣͤͥͦͧͨͩͪͫͬͭͮͯ͘͜͟͢͝͞͠͡ͰͱͲͳʹ͵Ͷͷ\\u0378\\u0379ͺͻͼͽ;Ϳ\\u0380\\u0381\\u0382\\u0383΄΅Ά·ΈΉΊ\\u038bΌ\\u038dΎΏΐΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡ\\u03a2ΣΤΥΦΧΨΩΪΫάέήίΰαβγδεζηθικλμνξοπρςστυφχψωϊϋόύώϏϐϑϒϓϔϕϖϗϘϙϚϛϜϝϞϟϠϡϢϣϤϥϦϧϨϩϪϫϬϭϮϯϰϱϲϳϴϵ϶ϷϸϹϺϻϼϽϾϿЀЁЂЃЄЅІЇЈЉЊЋЌЍЎЏАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяѐёђѓєѕіїјљњћќѝўџѠѡѢѣѤѥѦѧѨѩѪѫѬѭѮѯѰѱѲѳѴѵѶѷѸѹѺѻѼѽѾѿҀҁ҂҃҄҅҆҇҈҉ҊҋҌҍҎҏҐґҒғҔҕҖҗҘҙҚқҜҝҞҟҠҡҢңҤҥҦҧҨҩҪҫҬҭҮүҰұҲҳҴҵҶҷҸҹҺһҼҽҾҿӀӁӂӃӄӅӆӇӈӉӊӋӌӍӎӏӐӑӒӓӔӕӖӗӘәӚӛӜӝӞӟӠӡӢӣӤӥӦӧ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_start = 0x0100  # Start of Latin Extended-A block\n",
    "latin_end = latin_start + 1000  # 1000 characters from the start\n",
    "\n",
    "labels = ''.join(chr(i) for i in range(latin_start, latin_end))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9b0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Methods required to generate random ER graphs\n",
    "'''\n",
    "#Erdos Renyi graph\n",
    "def generate_er_graph(n, p, random_seed):\n",
    "    G = nx.erdos_renyi_graph(n,p,seed=random_seed)\n",
    "    M = nx.adjacency_matrix(G).todense()\n",
    "    return M, G\n",
    "\n",
    "'''\n",
    "Methods required to generate random sparse matrices\n",
    "'''\n",
    "def sprandsym(n, density,seed):\n",
    "    np.random.seed((seed))\n",
    "    rvs = stats.poisson(25, loc=10).rvs\n",
    "    X = sparse.random(n, n, density=density, data_rvs=rvs)\n",
    "    upper_X = sparse.triu(X)\n",
    "    result = upper_X + upper_X.T - sparse.diags(X.diagonal())\n",
    "    return result\n",
    "\n",
    "def binarize_sparse_matrix(sparse_matrix):\n",
    "    # create a copy of the sparse matrix to keep the operation non-destructive\n",
    "    sparse_copy = sparse_matrix.copy()\n",
    "    #sparse_copy=sparse_copy-sparse.diags(sparse_copy.diagonal())\n",
    "    # find the coordinates of non-zero elements\n",
    "    non_zero_coords = sparse_copy.nonzero()\n",
    "    # set those elements to 1\n",
    "    sparse_copy[non_zero_coords] = 1\n",
    "    return sparse_copy\n",
    "\n",
    "def generate_graph_from_matrix(binarized_sparse_mat):\n",
    "    G = nx.from_scipy_sparse_array(binarized_sparse_mat)\n",
    "    return G\n",
    "\n",
    "\n",
    "# create the quadratic program instance and define the variables\n",
    "def create_qp_from_qmatrix(Q_matrix):\n",
    "    max_keys = Q_matrix.shape[0]\n",
    "    qp = QuadraticProgram('QUBO Matrix Optimization')\n",
    "    x = qp.binary_var_list(name='x', keys=range(1, max_keys + 1))\n",
    "\n",
    "    linear_vars = {qp.get_variable(i).name: Q_matrix[i, j]\n",
    "                   for i in range(max_keys) for j in range(max_keys) if i == j}\n",
    "    quadratic_vars = {(qp.get_variable(i).name, qp.get_variable(j).name): Q_matrix[i, j]\n",
    "                      for i in range(max_keys) for j in range(max_keys) if i != j}\n",
    "\n",
    "    qp.minimize(linear=linear_vars, quadratic=quadratic_vars)\n",
    "    return qp\n",
    "    #print(self.qp.prettyprint())\n",
    "\n",
    "\n",
    "def create_qaoa_ansatz(qp):\n",
    "    #self.create_qp_from_qmatrix()\n",
    "    h_qubo, offset = qp.to_ising()\n",
    "    #print(h_qubo)\n",
    "    qaoa_ansatz = QAOAAnsatz(cost_operator=h_qubo, reps=1, )\n",
    "    qaoa_ansatz.entanglement = 'linear'\n",
    "    params = len(qaoa_ansatz.parameters)\n",
    "    theta_range = np.linspace(0, np.pi, params)\n",
    "    qaoa_qc = qaoa_ansatz.bind_parameters(theta_range)\n",
    "    decomposed_qaoa_ansatz = qaoa_qc.decompose().decompose().decompose().decompose()\n",
    "    return h_qubo, offset, decomposed_qaoa_ansatz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "745091e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph_properties1(G):\n",
    "    cnt=0\n",
    "    subgraphs = (G.subgraph(c) for c in nx.connected_components(G))\n",
    "    subgraph_prop = {}\n",
    "    prop = []\n",
    "    max_size = []\n",
    "    max_subgraph_nodes = ''\n",
    "    for s in subgraphs:\n",
    "        #print(s.nodes())\n",
    "        n = tuple(s.nodes())\n",
    "        subgraph_prop[n] = nx.adjacency_matrix(s).todense()\n",
    "        #print(s.size())\n",
    "        #print(f'Subgraph {cnt}:: Num of Edges: {s.size()},  Nodes : {s.nodes()}  ')\n",
    "        cnt+=1\n",
    "        max_size.append(len(s.nodes()))\n",
    "        if len(s.nodes)== np.max(max_size):\n",
    "            max_subgraph_nodes = s.nodes()\n",
    "        \n",
    "        \n",
    "    #print(max_subgraph_nodes)\n",
    "    return cnt, np.max(max_size), subgraph_prop, max_subgraph_nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884a7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(q1, max_cluster_size):\n",
    "    q1_nd = [q[0] for q in sorted(q1.degree, key=lambda x:x[1], reverse=True)]\n",
    "            \n",
    "    q_scidx = {}\n",
    "    scidx_q = {}\n",
    "    sc_idx = 0\n",
    "    visited = []\n",
    "\n",
    "    #for node in sorted(q1.nodes):\n",
    "    for node in q1_nd:\n",
    "        if node not in visited:\n",
    "            scidx_q.setdefault(sc_idx, [])\n",
    "\n",
    "            nodes_in_sc = set(list(sum(sorted(list(nx.bfs_tree(q1, source=node, depth_limit=1).edges())),())))\n",
    "            #print(f'nodes_in_sc: {nodes_in_sc}')\n",
    "\n",
    "            for k in nodes_in_sc:\n",
    "                ## required only if using to find cut wire position*****\n",
    "                q_scidx.setdefault(k,[]) #q_scidx\n",
    "                q_scidx[k].append(sc_idx) #q_scidx\n",
    "                ## ********\n",
    "\n",
    "                #print(f'visited: {visited}')\n",
    "                if (k not in visited) and (len(scidx_q[sc_idx])<max_cluster_size) :\n",
    "                    scidx_q[sc_idx].append(k)\n",
    "                    visited.append(k)\n",
    "            sc_idx += 1\n",
    "\n",
    "    qsc = OrderedDict(sorted(scidx_q.items()))\n",
    "    cluster_qubit ={}\n",
    "    bfs_cluster_labels = []\n",
    "\n",
    "    for qsc_k, qsc_val in qsc.items():\n",
    "        for qsc_i in qsc_val:\n",
    "            cluster_qubit.setdefault(qsc_i,[])\n",
    "            cluster_qubit[qsc_i].append(qsc_k)\n",
    "    bfs_cluster_labels = [i for val in (OrderedDict(sorted(cluster_qubit.items()))).values() for i in val]\n",
    "    return bfs_cluster_labels\n",
    "\n",
    "def sc(data, n_clusters, key, max_cluster_size):\n",
    "    sc = SpectralClustering(n_clusters, affinity='precomputed', n_init=100)\n",
    "    sc.fit(data)\n",
    "    #TODO: verify sub cluster size<max_cluster_size\n",
    "    return sc.labels_\n",
    "\n",
    "def kmeans(data, n_clusters):\n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=n_clusters, n_init=4, random_state=0)\n",
    "    kmeans.fit(data)\n",
    "    return kmeans.labels_\n",
    "    \n",
    "def kmeans_pca(data, n_clusters):\n",
    "    pca = PCA(n_components=n_clusters).fit(data)\n",
    "    kmeans_pca = KMeans(init=pca.components_, n_clusters=n_clusters, n_init=1)\n",
    "    kmeans_pca.fit(data)\n",
    "    return kmeans_pca.labels_\n",
    "\n",
    "def kmeans_random(data, n_clusters):\n",
    "    kmeans_rand = KMeans(init=\"random\", n_clusters=n_clusters, n_init=4, random_state=0)\n",
    "    kmeans_rand.fit(data)\n",
    "    return kmeans_rand.labels_\n",
    "\n",
    "def birch(data, n_clusters):\n",
    "    brc = Birch(n_clusters=n_clusters)\n",
    "    brc.fit(data)\n",
    "    birch_labels = brc.predict(data)\n",
    "    return birch_labels\n",
    "\n",
    "def agglom(data, n_clusters):\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters).fit(data)\n",
    "    return clustering.labels_\n",
    "\n",
    "def louivan_community(G,max_size):\n",
    "    import community as community_louvain\n",
    "    clabels = community_louvain.best_partition(G)\n",
    "\n",
    "    # Split communities if they are larger than max_cluster_size\n",
    "    return clabels #split_large_communities(clabels, max_size)\n",
    "\n",
    "def girvan_newman_community(G, max_size):\n",
    "    communities_generator = nx.community.girvan_newman(G)\n",
    "    top_level_communities = next(communities_generator)\n",
    "    partition = {node: i for i, community in enumerate(top_level_communities) for node in community}\n",
    "    return partition #split_large_communities(clabels, max_size)\n",
    "\n",
    "def label_propagation_community(G, max_size):\n",
    "    partition = nx.community.label_propagation_communities(G)\n",
    "    partition_dict = {node: i for i, community in enumerate(partition) for node in community}\n",
    "    return partition_dict #split_large_communities(partition_dict, max_size)\n",
    "\n",
    "def leading_eigenvector_community(G, max_size):\n",
    "    partition = nx.community.leading_eigenvector(G)\n",
    "    partition_dict = {node: i for i, community in enumerate(partition) for node in community}\n",
    "    return partition_dict #split_large_communities(partition_dict, max_size)\n",
    "\n",
    "def walktrap_community(G, max_size):\n",
    "    import igraph as ig\n",
    "    g = ig.Graph.from_networkx(G)\n",
    "    walktrap = g.community_walktrap()\n",
    "    communities = walktrap.as_clustering()\n",
    "    partition_dict = {node: i for i, community in enumerate(communities) for node in community}\n",
    "    return partition_dict #split_large_communities(partition_dict, max_size)\n",
    "\n",
    "def infomap_community(G, max_size):\n",
    "    from infomap import Infomap\n",
    "    im = Infomap()\n",
    "    for e in G.edges():\n",
    "        im.add_link(*e)\n",
    "    im.run()\n",
    "\n",
    "    # Correctly accessing the nodes and their community assignment\n",
    "    partition = {}\n",
    "    for node in im.tree:\n",
    "        if node.is_leaf:\n",
    "            partition[node.node_id] = node.module_id\n",
    "\n",
    "    return partition #split_large_communities(clabels, max_size)\n",
    "\n",
    "\n",
    "def clique_percolation_community(G,  max_size,k=2):\n",
    "    partition = list(nx.community.k_clique_communities(G, k))\n",
    "    partition_dict = {node: i for i, community in enumerate(partition) for node in community}\n",
    "    return partition_dict #split_large_communities(partition_dict, max_size)\n",
    "\n",
    "\n",
    "def split_large_communities(partition, max_size):\n",
    "    new_partition = {}\n",
    "    new_community_id = max(partition.values()) + 1  # Start from the next available community ID\n",
    "\n",
    "    for community in set(partition.values()):\n",
    "        nodes_in_community = [node for node, comm in partition.items() if comm == community]\n",
    "        \n",
    "        if len(nodes_in_community) > max_size:\n",
    "            # Split the community into smaller ones\n",
    "            for i in range(0, len(nodes_in_community), max_size):\n",
    "                for node in nodes_in_community[i:i + max_size]:\n",
    "                    new_partition[node] = new_community_id\n",
    "                new_community_id += 1\n",
    "        else:\n",
    "            # If the community is within the size limit, keep it unchanged\n",
    "            for node in nodes_in_community:\n",
    "                new_partition[node] = community\n",
    "\n",
    "    return new_partition\n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "'''def partitioning(max_cluster_size, qsubgraph_prop, clustering_method):\n",
    "    cm_part_lbl = {}\n",
    "    partitioning_time = {}\n",
    "    for cm in clustering_method:\n",
    "        part_lbl = {}\n",
    "        max_key_cnt = -1\n",
    "   \n",
    "        for i, key in enumerate(qsubgraph_prop.keys()):  \n",
    "            print(f'Subgraph nodes : {key}')\n",
    "            if len(key)>max_cluster_size:\n",
    "                data = qsubgraph_prop[key]\n",
    "                n_clusters = int(np.ceil(len(key)/max_cluster_size))\n",
    "                q1=nx.from_numpy_array(data)\n",
    "                \n",
    "                #call diff clustering methods here\n",
    "                if 'bfs' in cm:\n",
    "                    clabels = bfs(q1, max_cluster_size)\n",
    "\n",
    "                elif 'spectral_clustering' in cm:\n",
    "                    clabels = sc(data, n_clusters, key, max_cluster_size)\n",
    "                \n",
    "                elif cm=='kmeans':\n",
    "                    clabels = kmeans(data, n_clusters)\n",
    "                \n",
    "                elif cm=='kmeans_pca':\n",
    "                    clabels = kmeans_pca(data, n_clusters)\n",
    "\n",
    "                elif cm=='kmeans_random':\n",
    "                    clabels = kmeans_random(data, n_clusters)\n",
    "                \n",
    "                elif 'birch' in cm:\n",
    "                    clabels = birch(data, n_clusters)\n",
    "                \n",
    "                elif 'agglom' in cm:\n",
    "                    clabels = agglom(data, n_clusters)\n",
    "                    \n",
    "                elif 'louivan' in cm:\n",
    "                    clabels=louivan_community(q1,max_cluster_size)\n",
    "                    \n",
    "                elif 'girvan-newman' in cm:\n",
    "                    clabels=girvan_newman_community(q1,max_cluster_size)\n",
    "                    \n",
    "                elif 'label-propagation' in cm:\n",
    "                    clabels=label_propagation_community(q1,max_cluster_size)\n",
    "                    \n",
    "                elif 'leading-eigenvector' in cm:\n",
    "                    clabels=leading_eigenvector_community(q1,max_cluster_size)\n",
    "                    \n",
    "                elif 'walktrap' in cm:\n",
    "                    clabels=walktrap_community(q1,max_cluster_size)\n",
    "                    \n",
    "                elif 'infomap' in cm:\n",
    "                    clabels=infomap_community(q1,max_cluster_size)\n",
    "                    \n",
    "                elif 'clique_percolation' in cm:\n",
    "                    clabels=clique_percolation_community(q1,max_cluster_size)\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "                #increment the sclbl with max_key_cnt so that the next sub-graphs labels are not repeated\n",
    "                cluster_lbls = [lbl + max_key_cnt + 1 for lbl in clabels]\n",
    "                for j, k_ in enumerate(key):\n",
    "                    if cluster_lbls[j] < len(labels):\n",
    "                        part_lbl[k_] = labels[cluster_lbls[j]]\n",
    "                    else:\n",
    "                        print(f'Error: Index out of range. cluster_lbls[j]: {cluster_lbls[j]}, Length of labels: {len(labels)}')\n",
    "\n",
    "                max_key_cnt = np.max(cluster_lbls)\n",
    "\n",
    "            else:\n",
    "                # Handle smaller subgraphs\n",
    "                max_key_cnt += 1\n",
    "                for k in key:\n",
    "                    if max_key_cnt < len(labels):\n",
    "                        part_lbl[k] = labels[max_key_cnt]\n",
    "                    else:\n",
    "                        print(f'Error: Index out of range. max_key_cnt: {max_key_cnt}, Length of labels: {len(labels)}')\n",
    "\n",
    "        cm_part_lbl[cm] = (part_lbl, partitioning_runtime)\n",
    "        print(f'Partition labels for method {cm}: {part_lbl}')\n",
    "        print(f'Time taken for {cm}: {partitioning_time[cm]} seconds')\n",
    "\n",
    "    return cm_part_lbl'''\n",
    "\n",
    "\n",
    "def ckt(qaoa_decomposed, part_lbl, observables):\n",
    "    \n",
    "    ordered_part_lbl = OrderedDict(sorted(part_lbl.items()))\n",
    "    partition_labels = ''.join(ordered_part_lbl.values())\n",
    "    print(f'\\nPartition labels for CKT: {partition_labels}')\n",
    "    \n",
    "    partitioned_problem = partition_problem(circuit=qaoa_decomposed, \n",
    "                                            partition_labels=partition_labels, \n",
    "                                            observables=observables)\n",
    "    bases = partitioned_problem.bases\n",
    "    sampling_overhead = np.prod([basis.overhead for basis in bases])\n",
    "    print(f\"Sampling overhead: {sampling_overhead}\")\n",
    "    return partition_labels,sampling_overhead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "25166935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def partitioning(max_cluster_size, qsubgraph_prop, partition_method):\n",
    "    \n",
    "    print('in part')\n",
    "    #cm_part_lbl = {}\n",
    "    #partitioning_time = {}\n",
    "    start_time = time.time()\n",
    "    part_lbl = {}\n",
    "    max_key_cnt = -1\n",
    "    for i, key in enumerate(qsubgraph_prop.keys()):  \n",
    "        print(f'Subgraph nodes : {key}')\n",
    "        if len(key)>max_cluster_size:\n",
    "            data = qsubgraph_prop[key]\n",
    "            n_clusters = int(np.ceil(len(key)/max_cluster_size))\n",
    "            q1=nx.from_numpy_array(data)\n",
    "            partition_algos = {\n",
    "                #'bfs' : bfs(q1, max_cluster_size),\n",
    "                'spectral-clustering' : sc(data, n_clusters, key, max_cluster_size),\n",
    "                'kmeans' : kmeans(data, n_clusters),\n",
    "                #'kmeans-pca' : kmeans_pca(data, n_clusters),\n",
    "                #'kmeans-random' : kmeans_random(data, n_clusters),\n",
    "                #'birch' : birch(data, n_clusters),\n",
    "                #'agglom' : agglom(data, n_clusters),\n",
    "                #'louivan' : louivan_community(q1,max_cluster_size),\n",
    "                #'girvan-newman' : girvan_newman_community(q1,max_cluster_size),\n",
    "                #'label-propagation' : label_propagation_community(q1,max_cluster_size),\n",
    "                #'leading-eigenvecto' : leading_eigenvector_community(q1,max_cluster_size),\n",
    "                #'walktrap' : walktrap_community(q1,max_cluster_size),\n",
    "                #'infomap' : infomap_community(q1,max_cluster_size),\n",
    "                #'clique-percolation' : clique_percolation_community(q1,max_cluster_size)\n",
    "            }\n",
    "\n",
    "            #call the partition method\n",
    "            clabels = partition_algos[partition_method]\n",
    "\n",
    "            #increment the sclbl with max_key_cnt so that the next sub-graphs labels are not repeated\n",
    "            cluster_lbls = [lbl + max_key_cnt + 1 for lbl in clabels]\n",
    "            for j, k_ in enumerate(key):\n",
    "                if cluster_lbls[j] < len(labels):\n",
    "                    part_lbl[k_] = labels[cluster_lbls[j]]\n",
    "                else:\n",
    "                    print(f'Error: Index out of range. cluster_lbls[j]: {cluster_lbls[j]}, Length of labels: {len(labels)}')\n",
    "\n",
    "            max_key_cnt = np.max(cluster_lbls)\n",
    "\n",
    "        else:\n",
    "            # Handle smaller subgraphs\n",
    "            max_key_cnt += 1\n",
    "            for k in key:\n",
    "                if max_key_cnt < len(labels):\n",
    "                    part_lbl[k] = labels[max_key_cnt]\n",
    "                else:\n",
    "                    print(f'Error: Index out of range. max_key_cnt: {max_key_cnt}, Length of labels: {len(labels)}')\n",
    "        print('done part')\n",
    "        #cm_part_lbl[cm] = (part_lbl, partitioning_runtime)\n",
    "        #print(f'Partition labels for method {cm}: {part_lbl}')\n",
    "    partitioning_time = time.time() - start_time  \n",
    "\n",
    "    return part_lbl, partitioning_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fc98443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def ckt_build_qaoa(mat_size, n_times_p, random_seeds, matrix_type):\n",
    "    '''\n",
    "    ## Create sparse matrix for a given size and density\n",
    "    ## Convert the sparse matrix into QUBO, map Ising hamiltonian\n",
    "    ## Generate QAOA ansatz for QUBO and assign its observables\n",
    "    '''\n",
    "    \n",
    "    cols = ['n', 'n_times_p', 'seed','Graph Prop','qaoa Ansatz File', \n",
    "            'qaoa observables','Ising Hamiltonian Runtime', 'Ansatz Building Runtime']\n",
    "\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    df['Graph Prop'] = df['Graph Prop'].astype('object')\n",
    "   \n",
    "    i = 0\n",
    "    for n in mat_size:\n",
    "        for p in (n_times_p): \n",
    "            for seed in random_seeds:\n",
    "                #multi-thread code to go here for a set of seeds??\n",
    "                try:\n",
    "                    print(f'\\n\\nQAOA for size {n}, n*p {p}, seed {seed}')\n",
    "                    i+=1\n",
    "                    ## Create sparse matrix for a given size and density\n",
    "                    #record the matrix creation time\n",
    "                    start_time = time.time()\n",
    "                    if 'random_sparse' in matrix_type:\n",
    "                        M = sprandsym(n,p,seed)\n",
    "                        M = binarize_sparse_matrix(M)\n",
    "                        q=generate_graph_from_matrix(M)\n",
    "\n",
    "                    ## Get adjacency matrix for a random Erdos Renyi Graph\n",
    "                    elif 'er_graph' in matrix_type:\n",
    "                        M,q = generate_er_graph(n,p,seed)\n",
    "                    matrix_creation_time = time.time() - start_time\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    # save graph object to file\n",
    "                    graph_filename=f'{n}_{p}_{seed}_graph.pickle'\n",
    "                    pickle.dump(q, open(graph_filename, 'wb'))\n",
    "\n",
    "                    #Get subgraphs' properties\n",
    "                    qnum_sub_graphs, largest_subgraph_size, qsubgraph_prop, max_subgraph_nodes = get_subgraph_properties1(q)\n",
    "                \n",
    "                    \n",
    "                    ## Convert the sparse matrix into QUBO\n",
    "                    # Timing for QUBO conversion\n",
    "                    start_time = time.time()\n",
    "                    qp = create_qp_from_qmatrix(M)\n",
    "                    qp2qubo = QuadraticProgramToQubo()\n",
    "                    qubo = qp2qubo.convert(qp)\n",
    "                    qubitOp, offset = qubo.to_ising()\n",
    "                    qubo_conversion_time = time.time() - start_time\n",
    "\n",
    "                    ## Generate and save QAOA ansatz for QUBO\n",
    "                    # Timing for QAOA\n",
    "                    start_time = time.time()\n",
    "                    qaoa = QAOAAnsatz(cost_operator=qubitOp, reps=1)\n",
    "                    qaoa_observable_pat = '[A-Z]+'\n",
    "                    observables = PauliList(re.findall(qaoa_observable_pat, str(qubitOp)))\n",
    "                    qaoa_time = time.time() - start_time\n",
    "                    \n",
    "                    #pickle the qaoa ansatz\n",
    "                    qc_file = f'{n}_{p}_{seed}_qaoa.qpy'\n",
    "                    with open(file=qc_file, mode='wb') as qcfile:\n",
    "                        qpy.dump(qaoa, qcfile)\n",
    "                        \n",
    "                    #Populate the df\n",
    "                    df.loc[i,'n'] = n\n",
    "                    df.loc[i,'n_times_p'] = p\n",
    "                    df.loc[i,'seed'] = seed\n",
    "                    df.loc[i,'Graph Prop'] = graph_filename\n",
    "                    df.loc[i,'qaoa Ansatz File'] = qc_file\n",
    "                    df.loc[i,'qaoa observables'] = observables\n",
    "                    df.loc[i,'Ising Hamiltonian Runtime'] = qubo_conversion_time\n",
    "                    df.loc[i,'Ansatz Building Runtime'] = qaoa_time\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    # Optionally, save the DataFrame\n",
    "                    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    filename = f'exp_data_up_to_failure_{current_time}.csv'\n",
    "                    df.to_csv(filename, index=False)\n",
    "                    #re-raise the exception if you want to stop the loop\n",
    "                    raise e\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f'exp_' + str(n) + '_' + str(p) + '_' + str(seed) + '_qaoa_' +current_time+'.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ckt_cut_qaoa(exp_data_file, max_qubit_counts, partition_methods):\n",
    "    cols = ['n', 'n_times_p', 'seed','qaoa Ansatz File','Max Qubit Count',\n",
    "           'Partition Method', 'Partition Runtime', 'CKT Cutting Runtime']\n",
    "\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    input_df = pd.read_csv(exp_data_file)\n",
    "    print(input_df.dtypes)\n",
    "    i = 0\n",
    "    for max_qubit_cnt in max_qubit_counts:\n",
    "        #multi-processor code here\n",
    "        for i, row in enumerate(input_df.itertuples(index=False), 1):\n",
    "            n = row[0]\n",
    "            p = row[1]\n",
    "            seed = row[2]\n",
    "            graph_filename = row[3]\n",
    "            print(row[3])\n",
    "            \n",
    "            #import ast\n",
    "            #qsubgraph_prop = eval(qsub_prop)\n",
    "            \n",
    "            # load graph object from file\n",
    "            q = pickle.load(open(graph_filename, 'rb'))\n",
    "            #print(q)\n",
    "            qnum_sub_graphs, largest_subgraph_size, qsubgraph_prop, max_subgraph_nodes = get_subgraph_properties1(q)\n",
    "                \n",
    "            qc_file = row[4]\n",
    "            observables = row[5]\n",
    "            print(observables)\n",
    "            #read the file with qaoa ansatz object\n",
    "            st = time.time()\n",
    "            with open(qc_file, mode='rb') as qc:\n",
    "                reloaded_qaoa_ansatz = qpy.load(qc)[0]\n",
    "            qaoa_decomposed = reloaded_qaoa_ansatz.decompose().decompose().decompose().decompose()\n",
    "            reload_time = time.time() - st\n",
    "            print(reload_time)\n",
    "            try:\n",
    "                print(f'\\n\\nCKT Cutting for size {n}, n*p {p}, seed {seed}')\n",
    "                #Get subgraphs' properties\n",
    "                for pm_ in partition_methods:\n",
    "                    try:\n",
    "                        pm_part_lbl, partitioning_runtime = partitioning(max_qubit_cnt, \n",
    "                                                           qsubgraph_prop, \n",
    "                                                           pm_)\n",
    "                        print(pm_part_lbl)\n",
    "                        i += 1\n",
    "                        sc_partlabel,sampling_overhead,ckt_runtime = ckt(qaoa_decomposed, \n",
    "                                                                          pm_part_lbl, \n",
    "                                                                          observables)\n",
    "                        df.loc[i,'n'] = n\n",
    "                        df.loc[i,'p'] = n_times_p\n",
    "                        df.loc[i,'seed'] = seed\n",
    "                        df.loc[i,'Max Qubit Count'] = max_qubit_cnt\n",
    "                        df.loc[i,'Partition Method'] = pm_\n",
    "                        df.loc[i,'Partition Runtime'] = partitioning_runtime\n",
    "                        df.loc[i,'CKT Cutting Runtime'] = ckt_runtime\n",
    "                        df.loc[i,'Sampling Overhead'] = sampling_overhead\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        continue\n",
    "\n",
    "            except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f'exp_' + str(n) + '_' + str(p) + '_' + str(seed) + '_cktCut_' +current_time+'.csv'\n",
    "            df.to_csv(filename, index=False)\n",
    "                    \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fb791b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                              int64\n",
      "n_times_p                    float64\n",
      "seed                           int64\n",
      "Graph Prop                    object\n",
      "qaoa Ansatz File              object\n",
      "qaoa observables              object\n",
      "Ising Hamiltonian Runtime    float64\n",
      "Ansatz Building Runtime      float64\n",
      "dtype: object\n",
      "10_0.6_9488_graph.pickle\n",
      "['IIIIIIIIZZ', 'IIIIIIIIIZ', 'IIIIIIIIZI', 'IIIIIIZIIZ', 'IIIIIIZIII',\n",
      " 'IIIIIIZIZI', 'IIIIIZIIZI', 'IIIIIZIIII', 'IIIIIZIZII', 'IIIIIIIZII',\n",
      " 'IIIIIZZIII', 'IIIIZIZIII', 'IIIIZIIIII', 'IIIZIIIIIZ', 'IIIZIIIIII',\n",
      " 'IIIZIIIIZI', 'IIIZIIIZII', 'IIIZIZIIII', 'IIIZZIIIII', 'IIZIIIIIZI',\n",
      " 'IIZIIIIIII', 'IIZZIIIIII', 'IZIIIIIIIZ', 'IZIIIIIIII', 'IZIIIIZIII',\n",
      " 'IZIIIZIIII', 'IZIIZIIIII', 'IZIZIIIIII', 'ZIIIIIIIZI', 'ZIIIIIIIII',\n",
      " 'ZIIIIIIZII', 'ZIIIIZIIII', 'ZIIZIIIIII', 'ZZIIIIIIII']\n",
      "0.11506795883178711\n",
      "\n",
      "\n",
      "CKT Cutting for size 10, n*p 0.6, seed 9488\n",
      "in part\n",
      "Subgraph nodes : (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
      "'spectral_clustering'\n",
      "in part\n",
      "Subgraph nodes : (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
      "done part\n",
      "{0: 'Ā', 1: 'ā', 2: 'ā', 3: 'Ă', 4: 'ă', 5: 'Ā', 6: 'Ă', 7: 'Ā', 8: 'ā', 9: 'ă'}\n",
      "\n",
      "Partition labels for CKT: ĀāāĂăĀĂĀāă\n",
      "An input observable acts on a different number of qubits than the input circuit.\n"
     ]
    }
   ],
   "source": [
    "ckt_cut_qaoa('exp_10_0.6_9488_qaoa_20240204_214424.csv', [3], ['spectral_clustering','kmeans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7cdb919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "QAOA for size 10, n*p 0.6, seed 9488\n"
     ]
    }
   ],
   "source": [
    "matrix_sizes =  [10]\n",
    "\n",
    "matrix_densities = [0.6]\n",
    "num_of_experiments = 1\n",
    "random_seeds = [random.randint(3000, 10000) for _ in range(num_of_experiments)]\n",
    "max_cluster_sizes =[3] \n",
    "\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "df = ckt_build_qaoa(matrix_sizes, matrix_densities, random_seeds, 'er_graph')\n",
    "endtime = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a984a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Qiskit/qiskit/pull/5578\n",
    "#qaoa.qasm(formatted=True, filename=qc_file)\n",
    "                    \n",
    "from qiskit.qpy import dump, load\n",
    "import io\n",
    "'''qpy_file = io.BytesIO()\n",
    "dump(qaoa,qpy_file)\n",
    "qpy_file.seek(0)\n",
    "new_circ = load(qpy_file)[0]\n",
    "display(new_circ.draw())'''\n",
    "\n",
    "\n",
    "M,q = generate_er_graph(10,0.6,32)\n",
    "qp = create_qp_from_qmatrix(M)\n",
    "qp2qubo = QuadraticProgramToQubo()\n",
    "qubo = qp2qubo.convert(qp)\n",
    "qubitOp, offset = qubo.to_ising()\n",
    "\n",
    "## Generate and save QAOA ansatz for QUBO\n",
    "# Timing for QAOA\n",
    "start_time = time.time()\n",
    "qaoa = QAOAAnsatz(cost_operator=qubitOp, reps=1)\n",
    "qaoa_time = time.time() - start_time\n",
    "\n",
    "#qc_file = f'{n}_{p}_{seed}_qaoa.qasm'\n",
    "#qaoa.qasm(formatted=True, filename=qc_file)\n",
    "\n",
    "#https://github.com/Qiskit/qiskit/pull/5578\n",
    "from qiskit.qpy import dump, load\n",
    "import io\n",
    "#qpy_file = io.BytesIO()\n",
    "dump(qaoa,qpy_file)\n",
    "with open(file='qpy_file', mode='wb') as qcfile:\n",
    "    dump(qaoa, qcfile)\n",
    "\n",
    "#load\n",
    "qpy_file.seek(0)\n",
    "new_circ = load(qpy_file)[0]\n",
    "#new_circ.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd003cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c27f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
